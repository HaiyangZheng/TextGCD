2024-12-06 22:44:37,373 - INFO - Training pets_lr(all/2)_seed0_pseudo(taus)_notransform_warmup(12)_align(3) with the following settings:
2024-12-06 22:44:37,374 - INFO - Command-line arguments: output_dir=exp_final_test
 experiment_name=pets_lr(all/2)_seed0_pseudo(taus)_notransform_warmup(12)_align(3)
 seed=0
 evaluate=False
 dataset_name=pets
 backbone_name=ViT-B/16
 cub_root=/leonardo_work/IscrC_Fed-GCD/GCD_datasets/CUB
 cub_retrieved_text_path=retrieved_text/cub_retrieved_text.npy
 flowers_root=/leonardo_work/IscrC_Fed-GCD/GCD_datasets
 flowers_retrieved_text_path=retrieved_text/flowers_retrieved_text.npy
 scars_root=/leonardo_work/IscrC_Fed-GCD/GCD_datasets
 scars_retrieved_text_path=retrieved_text/scars_retrieved_text.npy
 pets_root=/leonardo_work/IscrC_Fed-GCD/GCD_datasets
 pets_retrieved_text_path=retrieved_text/pets_retrieved_text.npy
 cifar10_root=/leonardo_work/IscrC_Fed-GCD/GCD_datasets/cifar10
 cifar10_retrieved_text_path=retrieved_text/cifar10_retrieved_text.npy
 cifar100_root=/leonardo_work/IscrC_Fed-GCD/GCD_datasets/cifar100
 cifar100_retrieved_text_path=retrieved_text/cifar100_retrieved_text.npy
 imagenet_root=/home/fandral/Nan/NCD_dataset
 imagenet_retrieved_text_path=retrieved_text/imagenet100_retrieved_text.npy
 osr_split_dir=data/ssb_splits
 epochs=200
 base_lr=0.00025
 classifier_lr=0.05
 momentum=0.9
 weight_decay=0.0001
 num_workers=8
 batch_size=128
 prop_train_labels=0.5
 use_ssb_splits=True
 transform=imagenet
 n_views=2
 selecting_ratio=0.6
 lambda_loss=0.2
 warm_up_epochs=12
 class_aligning_epochs=3
 num_attributes=2
 num_tags=3
 tau_s=0.1
 tau_u=0.05
 tau_t_start=0.035
 tau_t_end=0.02
 warmup_teacher_temp_epochs=30
 memax_weight=2
 image_size=224
 train_classes=range(0, 19)
 unlabeled_classes=range(19, 37)
 num_labeled_classes=19
 num_unlabeled_classes=18
 log_path=exp_final_test/pets_lr(all/2)_seed0_pseudo(taus)_notransform_warmup(12)_align(3)-2024-12-06-22-44/logs/log.txt
 model_path=exp_final_test/pets_lr(all/2)_seed0_pseudo(taus)_notransform_warmup(12)_align(3)-2024-12-06-22-44/models/model.pth
 device=cuda
2024-12-06 22:44:37,378 - INFO - Loading CLIP (backbone: ViT-B/16)
2024-12-06 22:44:39,740 - INFO - Building custom CLIP
2024-12-06 22:44:40,834 - INFO - Turning off gradients in both the image and the text encoder
2024-12-06 22:44:40,836 - INFO - Parameters that require gradients: ['model.text_projection', 'model.visual.proj', 'model.visual.transformer.resblocks.11.attn.in_proj_weight', 'model.visual.transformer.resblocks.11.attn.in_proj_bias', 'model.visual.transformer.resblocks.11.attn.out_proj.weight', 'model.visual.transformer.resblocks.11.attn.out_proj.bias', 'model.visual.transformer.resblocks.11.ln_1.weight', 'model.visual.transformer.resblocks.11.ln_1.bias', 'model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'model.visual.transformer.resblocks.11.ln_2.weight', 'model.visual.transformer.resblocks.11.ln_2.bias', 'model.transformer.resblocks.11.attn.in_proj_weight', 'model.transformer.resblocks.11.attn.in_proj_bias', 'model.transformer.resblocks.11.attn.out_proj.weight', 'model.transformer.resblocks.11.attn.out_proj.bias', 'model.transformer.resblocks.11.ln_1.weight', 'model.transformer.resblocks.11.ln_1.bias', 'model.transformer.resblocks.11.mlp.c_fc.weight', 'model.transformer.resblocks.11.mlp.c_fc.bias', 'model.transformer.resblocks.11.mlp.c_proj.weight', 'model.transformer.resblocks.11.mlp.c_proj.bias', 'model.transformer.resblocks.11.ln_2.weight', 'model.transformer.resblocks.11.ln_2.bias', 'image_classifier.weight_g', 'image_classifier.weight_v', 'text_classifier.weight_g', 'text_classifier.weight_v']
2024-12-06 22:44:41,805 - INFO - len of train dataset: 3680
2024-12-06 22:44:41,806 - INFO - len of test dataset: 2738
2024-12-06 22:44:41,810 - INFO - Parameters in classifier with big lr: ['image_classifier.weight_g', 'image_classifier.weight_v', 'text_classifier.weight_g', 'text_classifier.weight_v']
2024-12-06 22:44:41,810 - INFO - Selecting 44 high-confidence samples for co-teaching.
2024-12-06 22:44:58,159 - INFO - Before Train Accuracies: All 0.1622 | Old 0.0583 | New 0.2167
2024-12-06 22:44:58,159 - INFO - Before Train Accuracies: All 0.1300 | Old 0.0456 | New 0.1744
2024-12-06 22:45:25,522 - INFO - Epoch 1/200, Total Loss: 8.5755, Base Loss: 6.5612, Con Loss: 2.0143, Pseudo Loss Image: 0.0000, Pseudo Loss Text: 0.0000
2024-12-06 22:45:25,522 - INFO -    Param Group: classifier_head, Learning Rate: 0.0500
2024-12-06 22:45:25,522 - INFO -    Param Group: base_parameters, Learning Rate: 0.0003
2024-12-06 22:45:35,768 - INFO - Text classifier Epoch 0 Train Accuracies: All 0.5358 | Old 0.4751 | New 0.5677
2024-12-06 22:45:35,768 - INFO - Image classifier Epoch 0 Train Accuracies: All 0.2969 | Old 0.2142 | New 0.3404
2024-12-06 22:45:46,122 - INFO - Weighted Accuracies: All 0.3923 | Old 0.3796 | New 0.3989
2024-12-06 22:45:46,720 - INFO - Saved model!
2024-12-06 22:46:12,558 - INFO - Epoch 2/200, Total Loss: 6.8322, Base Loss: 4.9134, Con Loss: 1.9188, Pseudo Loss Image: 0.0000, Pseudo Loss Text: 0.0000
2024-12-06 22:46:12,558 - INFO -    Param Group: classifier_head, Learning Rate: 0.0500
2024-12-06 22:46:12,558 - INFO -    Param Group: base_parameters, Learning Rate: 0.0003
