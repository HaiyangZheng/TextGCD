2024-11-29 22:57:49,675 - INFO - Training scars_seed4_adda_capital with the following settings:
2024-11-29 22:57:49,676 - INFO - Command-line arguments: output_dir=exp
 experiment_name=scars_seed4_adda_capital
 seed=4
 evaluate=False
 dataset_name=scars
 backbone_name=ViT-B/16
 cub_root=/leonardo_work/IscrC_Fed-GCD/GCD_datasets/CUB
 cub_retrieved_text_path=retrieved_text/cub_retrieved_text.npy
 flowers_root=/leonardo_work/IscrC_Fed-GCD/GCD_datasets
 flowers_retrieved_text_path=retrieved_text/flowers_retrieved_text.npy
 scars_root=/leonardo_work/IscrC_Fed-GCD/GCD_datasets
 scars_retrieved_text_path=retrieved_text/scars_retrieved_text.npy
 pets_root=/leonardo_work/IscrC_Fed-GCD/GCD_datasets
 pets_retrieved_text_path=retrieved_text/pets_retrieved_text.npy
 cifar10_root=/leonardo_work/IscrC_Fed-GCD/GCD_datasets/cifar10
 cifar10_retrieved_text_path=retrieved_text/cifar10_retrieved_text.npy
 cifar100_root=/leonardo_work/IscrC_Fed-GCD/GCD_datasets/cifar100
 cifar100_retrieved_text_path=retrieved_text/cifar100_retrieved_text.npy
 imagenet_root=/home/fandral/Nan/NCD_dataset
 imagenet_retrieved_text_path=retrieved_text/imagenet100_retrieved_text.npy
 osr_split_dir=data/ssb_splits
 epochs=200
 base_lr=0.0005
 classifier_lr=0.1
 momentum=0.9
 weight_decay=0.0001
 num_workers=8
 batch_size=128
 prop_train_labels=0.5
 use_ssb_splits=True
 transform=imagenet
 n_views=2
 selecting_ratio=0.6
 lambda_loss=0.2
 warm_up_epochs=10
 class_aligning_epochs=5
 num_attributes=2
 num_tags=3
 tau_s=0.1
 tau_u=0.05
 tau_t_start=0.035
 tau_t_end=0.02
 warmup_teacher_temp_epochs=30
 memax_weight=2
 image_size=224
 train_classes=[1, 11, 25, 38, 46, 50, 53, 75, 84, 100, 105, 117, 123, 129, 133, 134, 135, 136, 137, 138, 140, 144, 145, 146, 147, 149, 150, 151, 153, 160, 161, 162, 163, 164, 167, 168, 169, 174, 175, 180, 185, 186, 187, 192, 193, 0, 81, 97, 104, 122, 139, 141, 142, 143, 148, 152, 154, 155, 156, 157, 158, 159, 165, 166, 170, 171, 172, 173, 176, 177, 181, 184, 188, 191, 194, 195, 2, 7, 9, 16, 20, 26, 28, 44, 54, 95, 98, 102, 127, 178, 182, 22, 41, 82, 93, 112, 125, 189]
 unlabeled_classes=[23, 42, 83, 94, 113, 126, 190, 3, 8, 10, 17, 21, 27, 29, 45, 55, 96, 99, 103, 128, 179, 183, 4, 5, 6, 12, 13, 14, 15, 18, 19, 24, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 43, 47, 48, 49, 51, 52, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 101, 106, 107, 108, 109, 110, 111, 114, 115, 116, 118, 119, 120, 121, 124, 130, 131, 132]
 num_labeled_classes=98
 num_unlabeled_classes=98
 log_path=exp/scars_seed4_adda_capital-2024-11-29-22-57/logs/log.txt
 model_path=exp/scars_seed4_adda_capital-2024-11-29-22-57/models/model.pth
 device=cuda
2024-11-29 22:57:49,681 - INFO - Loading CLIP (backbone: ViT-B/16)
2024-11-29 22:57:52,094 - INFO - Building custom CLIP
2024-11-29 22:57:53,211 - INFO - Turning off gradients in both the image and the text encoder
2024-11-29 22:57:53,213 - INFO - Parameters that require gradients: ['model.text_projection', 'model.visual.proj', 'model.visual.transformer.resblocks.11.attn.in_proj_weight', 'model.visual.transformer.resblocks.11.attn.in_proj_bias', 'model.visual.transformer.resblocks.11.attn.out_proj.weight', 'model.visual.transformer.resblocks.11.attn.out_proj.bias', 'model.visual.transformer.resblocks.11.ln_1.weight', 'model.visual.transformer.resblocks.11.ln_1.bias', 'model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'model.visual.transformer.resblocks.11.ln_2.weight', 'model.visual.transformer.resblocks.11.ln_2.bias', 'model.transformer.resblocks.11.attn.in_proj_weight', 'model.transformer.resblocks.11.attn.in_proj_bias', 'model.transformer.resblocks.11.attn.out_proj.weight', 'model.transformer.resblocks.11.attn.out_proj.bias', 'model.transformer.resblocks.11.ln_1.weight', 'model.transformer.resblocks.11.ln_1.bias', 'model.transformer.resblocks.11.mlp.c_fc.weight', 'model.transformer.resblocks.11.mlp.c_fc.bias', 'model.transformer.resblocks.11.mlp.c_proj.weight', 'model.transformer.resblocks.11.mlp.c_proj.bias', 'model.transformer.resblocks.11.ln_2.weight', 'model.transformer.resblocks.11.ln_2.bias', 'image_classifier.weight_g', 'image_classifier.weight_v', 'text_classifier.weight_g', 'text_classifier.weight_v']
2024-11-29 22:57:54,104 - INFO - len of train dataset: 8144
2024-11-29 22:57:54,104 - INFO - len of test dataset: 6144
2024-11-29 22:57:54,109 - INFO - Parameters in classifier with big lr: ['image_classifier.weight_g', 'image_classifier.weight_v', 'text_classifier.weight_g', 'text_classifier.weight_v']
2024-11-29 22:57:54,109 - INFO - Selecting 18 high-confidence samples for co-teaching.
2024-11-29 22:58:23,286 - INFO - Before Train Accuracies: All 0.0669 | Old 0.0600 | New 0.0702
2024-11-29 22:58:23,287 - INFO - Before Train Accuracies: All 0.0736 | Old 0.0350 | New 0.0922
2024-11-29 22:59:20,738 - INFO - Epoch 1/200, Total Loss: 11.8160, Base Loss: 10.0174, Con Loss: 1.7986, Pseudo Loss Image: 0.0000, Pseudo Loss Image: 0.0000
2024-11-29 22:59:20,738 - INFO -    Param Group: classifier_head, Learning Rate: 0.1000
2024-11-29 22:59:20,739 - INFO -    Param Group: base_parameters, Learning Rate: 0.0005
2024-11-29 22:59:42,768 - INFO - Text classifier Epoch 0 Train Accuracies: All 0.3346 | Old 0.3073 | New 0.3478
2024-11-29 22:59:42,769 - INFO - Image classifier Epoch 0 Train Accuracies: All 0.1908 | Old 0.1979 | New 0.1873
2024-11-29 23:00:04,825 - INFO - Weighted Accuracies: All 0.2726 | Old 0.3403 | New 0.2399
